{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Welcome to the 'Reasoning with LLMs Workshop!\n",
        "\n",
        "## Introduction\n",
        "In this workshop we will learn what are and how to build large language ‘reasoning’ models. After the OpenAI’s o-family of models, Gemini Thinking models and recent Grok3 models releases, so called ‘thinking’ or ‘reasoning’ LLMs became the latest buzz in the AI community. The DeepSeek family of models recently caused a lot of attention and their models have been open sourced as well as their training recipes. During the workshop we will demystify how these models are trained and use open source models and frameworks to replicate some of the concepts used in DeepSeek training.\n",
        "\n",
        "We will start with the model that is not particularly good at math and by using the publicly available models, datasets and frameworks, we will perform supervised finetuning (SFT) and Reinforcement Learning methods to train a reasoning model capable of solving math word problems.\n",
        "\n",
        "We will learn what are reasoning models, theory behind them, explain how and why Chain-of-Thought works, how to preprocess data, evaluate model, leverage techniques like, quantization and lora to train big models with a very small resource footprint (for GPU poor), track experiments and push models to HugginFace hub, so you will ‘take home’ your own reasoning LLM!\n",
        "\n",
        "During the workshop we will gradually build our own library for learning, exploring and playing with Reasoning (or Thinking) LLMs. We will work through notebooks, but at the same time, we will export important pieces of code into the separate .py files and thus build a library that is easy to use later and build upon (and please, feel free to do so after the workshop!). For example, you can try other models, datasets, reward functions etc.\n",
        "\n",
        "This is the folder structure that we will create:\n",
        "\n",
        "```\n",
        "reasoning_workshop/\n",
        "├── notebooks/\n",
        "│   ├── reasoning_llms_workshop.ipynb\n",
        "│   ├── setup.ipynb\n",
        "│\n",
        "├── src/\n",
        "│   ├── __init__.py\n",
        "│   ├── data_preparation.py\n",
        "│   ├── utils.py\n",
        "│   ├── evaluation.py\n",
        "│   └── reward_functions.py\n",
        "│\n",
        "├── scripts/\n",
        "│   ├── run_sft_training.py\n",
        "│   └── run_grpo_training.py\n",
        "│\n",
        "├── outputs/\n",
        "│   ├── sft_model/\n",
        "│   └── grpo_model/\n",
        "│\n",
        "├── requirements.txt\n",
        "└── README.md\n",
        "```\n",
        "\n",
        "LLM Reasoning is a controversial topic, but before we jump into why is it so (and discuss whether LLMs actually reason at all !?), let's get our hands dirty by tinkering with an actual model. There will be time for discussion when our models will be in training, evaluation etc.\n",
        "\n",
        "IMPORTANT - please save the copy of the notebook in your Google Drive. You can click on 'File' -> 'Save a copy in Drive' in the main menu. This will allow you to place all files and models in the folder of your Drive so you can easily access them later, convert to your own GitHub repo etc."
      ],
      "metadata": {
        "id": "FjRJ9G17rIlh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pW4OXcYyUYQq"
      },
      "outputs": [],
      "source": [
        "# --- Initial Project Setup ---\n",
        "# This cell creates the directory structure for our project.\n",
        "# We'll be populating these files as we go through the workshop.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Add the path to your project folder in Drive to Python's search path\n",
        "directory = '/content/drive/MyDrive/Colab_Notebooks/llm_workshop'\n",
        "sys.path.append(directory)\n",
        "file_path = f\"{directory}/utils_hello_drive.py\"\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Subdirs\n",
        "!mkdir {directory}/notebooks\n",
        "!mkdir {directory}/scripts\n",
        "!mkdir {directory}/src\n",
        "!mkdir {directory}/outputs\n",
        "!mkdir {directory}/outputs/sft_model\n",
        "!mkdir {directory}/outputs/grpo_model\n",
        "!mkdir {directory}/models"
      ],
      "metadata": {
        "id": "3E6tPxIBl8vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the __init__.py file to make 'src' a Python package\n",
        "!touch {directory}/src/__init__.py\n"
      ],
      "metadata": {
        "id": "rUpjYb9Mo11V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {directory}/src/utils.py"
      ],
      "metadata": {
        "id": "Uv5INZAHrfeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's try to put some .py file in out ```src``` folder and then try to use it in our notebook with ```import```"
      ],
      "metadata": {
        "id": "wGgZGUgGscO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile  {directory}/hello_from_drive.py\n",
        "\n",
        "def hello_world():\n",
        "  print(\"Hello from our python scrip in Drive.py!\")"
      ],
      "metadata": {
        "id": "pPkdgfCBpM7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hello_from_drive\n",
        "\n",
        "hello_from_drive.hello_world()"
      ],
      "metadata": {
        "id": "ZG27hODts-3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's do all our neccessary installations and imports for playing with LLMs"
      ],
      "metadata": {
        "id": "UeeYYQuVuJCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uv"
      ],
      "metadata": {
        "collapsed": true,
        "id": "r-ylvP1xu-Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install unsloth vllm\n",
        "!pip install --upgrade datasets\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "joxcogGSupe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a model"
      ],
      "metadata": {
        "id": "iyD48P-HMEIn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkIvEkIIkEyB"
      },
      "outputs": [],
      "source": [
        "import unsloth\n",
        "from unsloth import FastModel\n",
        "max_seq_length = 1024\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-1b-it\", # \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\" for 4-bit version\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False,\n",
        "    load_in_8bit = False,\n",
        "    full_finetuning = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nHQVsQJ2roh"
      },
      "source": [
        "We now add LoRA adapters so we only need to update a small amount of parameters!\n",
        "\n",
        "LoRA is the Parameter Efficient Fine Tuning (PEFT) method. You can read more about it here:\n",
        "\n",
        "*   https://arxiv.org/abs/2106.09685\n",
        "*   https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNuwc5sJ2pYK"
      },
      "outputs": [],
      "source": [
        "# if we are about to train a model, let's add some lora adapters\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # Turn off for just text!\n",
        "    finetune_language_layers   = True,  # Should leave on!\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules       = True,  # Should leave on always!\n",
        "\n",
        "    r = 8,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 8,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's test our model with some generations. Note that here we are using 'streaming' generation mode."
      ],
      "metadata": {
        "id": "Xm9ldWspGNm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"solve this math problem.\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    tokenize = False,\n",
        ")\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 64, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
        ")"
      ],
      "metadata": {
        "id": "vg_Y3lDHNSLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "EUdeR5z-o0PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nwByzMRvxhr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generate some responses from our model (without streaming)."
      ],
      "metadata": {
        "id": "mH6wWmYd5Rmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time in a land far, far away, there lived a\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n"
      ],
      "metadata": {
        "id": "lgnOwE9e4xWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=50,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.7,\n",
        "    output_scores=True,\n",
        "    return_dict_in_generate=True,\n",
        "    num_return_sequences=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "AeHt9JG95x70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outputs.scores is a tuple of tensors, one per generation step\n",
        "# Each tensor shape: (batch_size, vocab_size)\n",
        "print(f\"Number of generation steps: {len(outputs.scores)}\")\n",
        "output_scores = outputs.scores # Logits for each generation step\n",
        "\n",
        "# Convert logits to probabilities\n",
        "# Get probabilities for the first generated token\n",
        "first_step_probs = torch.softmax(output_scores[0], dim=-1)\n",
        "top_k_probs, top_k_indices = torch.topk(first_step_probs, k=5)\n",
        "\n",
        "print(\"\\nTop 5 tokens and probabilities for the first generated token:\")\n",
        "for i in range(5):\n",
        "    token = tokenizer.decode(top_k_indices[0, i])\n",
        "    prob = top_k_probs[0, i].item()\n",
        "    print(f\"- Token: '{token}', Probability: {prob:.4f}\")"
      ],
      "metadata": {
        "id": "T5oItArC7FwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0]"
      ],
      "metadata": {
        "id": "5zRXXQ808Ikg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(outputs[0][0], skip_special_tokens=False)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "ot5b-vhD7MuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, models are probabilistic in nature, i.e. they autoregressively generate probabilities for the next token. We can influence this process with generation parameters like top_p, top_k, temperature etc.\n",
        "Keep this in mind when we discuss 'reasoning'."
      ],
      "metadata": {
        "id": "Pzf9nPYMibn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a dataset"
      ],
      "metadata": {
        "id": "5zPyrpjPyc1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import torch"
      ],
      "metadata": {
        "id": "aamhRHuMDzXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset(\"gsm8k\", \"main\", split=\"train\")\n",
        "test_dataset = load_dataset(\"gsm8k\", \"main\", split=\"test\")"
      ],
      "metadata": {
        "id": "oac4tvQBErlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "kPOoVekHURnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset[0]['question'], test_dataset[0]['answer']"
      ],
      "metadata": {
        "id": "8hXa8ZoDEvQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard Prompt\n",
        "prompt = \"Natalia sold 48 pastries in the morning and 23 pastries in the afternoon. She baked a total of 100 pastries. How many pastries did she have left?\"\n"
      ],
      "metadata": {
        "id": "AH_aJUPPCcP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prepare the input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "# Generate text\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens = 256,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.7,\n",
        "    output_scores=True,\n",
        "    return_dict_in_generate=True,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id # Ensure the model stops at the end of the sequence\n",
        ")\n"
      ],
      "metadata": {
        "id": "kyDlbfibcksK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(outputs[0][0], skip_special_tokens=False)\n",
        "print(\"\\nGenerated Text:\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "oafM_sjiIT6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the ```num_return_sequences``` parameter to a number greater than one will result in the model generating multiple different outputs for the same initial prompt.\n",
        "\n",
        "It is significantly faster to generate multiple sequences in a single execution by setting ```num_return_sequences``` than it is to run the same code multiple times in a loop.\n",
        "\n",
        "When we set ```num_return_sequences``` to a value like 5, we are instructing the model to produce five independent continuations of the initial prompt. Since our code uses do_sample=True, the model employs a sampling strategy (specifically, top-k and top-p sampling) to choose the next word at each step. This inherent randomness in the selection process allows for the generation of diverse sequences from the same starting point.\n",
        "\n",
        "If we were using a deterministic method (e.g., do_sample=False for greedy search), all the returned sequences would be identical. However, with sampling enabled, each of the returned sequences represents a different path the model has explored.\n",
        "\n",
        "Setting num_return_sequences to a higher value is more performant than iterating through the generation process for two primary reasons:\n",
        "\n",
        "Reduced Overhead: Each call to model.generate() involves a certain amount of overhead. This includes preparing the inputs, moving data to the GPU (if applicable), and initializing the generation process. Running the code in a loop incurs this overhead with every single iteration. A single call with multiple return sequences minimizes this repeated overhead.\n",
        "\n",
        "The most significant speed-up comes from batched computation. When you request multiple sequences in one go, the model can process the initial prompt and subsequent generation steps for all sequences in parallel on hardware like GPUs or TPUs. Modern deep learning models are highly optimized for this kind of parallel, batched computation. In contrast, a for loop that calls the generation function repeatedly processes each sequence sequentially, failing to take full advantage of the underlying hardware's parallel capabilities."
      ],
      "metadata": {
        "id": "533ZJqof2kjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's put generation code inside a function that we can simply reuse later on. Also, let's save this function in utils.py file - we are starting to build our small library!"
      ],
      "metadata": {
        "id": "vbgLuxaaN9UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile {directory}/utils.py\n",
        "import torch\n",
        "def generate_output(model, tokenizer, prompt: str,  **generation_kwargs) -> list[str]:\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "      prompt_token_count = inputs.input_ids.shape[1]\n",
        "\n",
        "      # This makes the function flexible; you can override defaults on-the-fly.\n",
        "      default_kwargs = {\n",
        "          \"max_new_tokens\": 256,\n",
        "          \"do_sample\": True,\n",
        "          \"top_k\": 64,\n",
        "          \"top_p\": 0.95,\n",
        "          \"temperature\": 1,\n",
        "          \"num_return_sequences\": 1,\n",
        "          \"pad_token_id\": tokenizer.eos_token_id,\n",
        "          \"eos_token_id\": tokenizer.eos_token_id, # good practice to set this\n",
        "      }\n",
        "\n",
        "      # The following line with update default generation parameters\n",
        "      default_kwargs.update(generation_kwargs)\n",
        "\n",
        "      outputs = model.generate(\n",
        "          **inputs,\n",
        "          **default_kwargs\n",
        "      )\n",
        "\n",
        "    # Decode and slice each sequence to get only the generated text\n",
        "    generated_texts = []\n",
        "    for sequence in outputs:\n",
        "        generated_tokens = sequence[prompt_token_count:]\n",
        "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=False)\n",
        "        generated_texts.append(generated_text.strip())\n",
        "\n",
        "    return generated_texts"
      ],
      "metadata": {
        "id": "WNPVeaxMKI1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import utils\n",
        "#import importlib\n",
        "#importlib.reload(utils) # Reload the utils module if you have problems loading it after modification\n",
        "\n",
        "#utils.generate_output(model, tokenizer, prompt, num_return_sequences=5)\n",
        "generate_output(model, tokenizer, prompt, num_return_sequences=5)"
      ],
      "metadata": {
        "id": "0k3Xe74ILQpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try running the cell above several times. **It seems like out model is not reliably following our instruction!**\n",
        "This is where finetuning (or Supervised FuneTuning - SFT) can help.\n",
        "Also, this is a good point to discuss different types of prompting techniques and how they influence the 'reasoning' capabilities.\n",
        "\n",
        "\n",
        "*   Few-shot prompting\n",
        "*   Chain of thought (CoT)\n",
        "\n",
        "But, before we jump into that, there is one more thing to check!\n",
        "Let's look at the official Gemma documentation. Try to find out if there is anything we missed!"
      ],
      "metadata": {
        "id": "z9MjDX3t0ubU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spoiler alert\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "n6B1_Omcl3ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemma official documentation (https://ai.google.dev/gemma/docs/core/prompt-structure) says this:\n",
        "\n",
        "\n",
        "\n",
        "> \\<start_of_turn>user\n",
        "knock knock\\<end_of_turn>\n",
        "\\<start_of_turn>model\n",
        "who is there\\<end_of_turn>\n",
        "\\<start_of_turn>user\n",
        "Gemma\\<end_of_turn>\n",
        "\\<start_of_turn>model\n",
        "Gemma who?\\<end_of_turn>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Gemma Techical Report: https://arxiv.org/html/2503.19786v1"
      ],
      "metadata": {
        "id": "m4TlpZrhKKmx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = train_dataset[0]['question']\n",
        "instruction = 'Solve this math problem step by step. After step by step solution write out #### followed with the number solution. '\n",
        "prompt = \"<bos><start_of_turn>user \" + instruction + prompt + \"<end_of_turn><start_of_turn>model \"\n",
        "prompt\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zAkL8ARyQdcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can make it more flexible with preamble and suffix\n",
        "preamble = \"<bos><start_of_turn>user \"\n",
        "suffix = \"<end_of_turn><start_of_turn>model \"\n",
        "prompt = preamble + instruction + train_dataset[0]['question'] + suffix\n",
        "prompt"
      ],
      "metadata": {
        "id": "1ZUu_2JDOxFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "outputs = utils.generate_output(model, tokenizer, prompt, num_return_sequences=5, max_new_tokens=512)\n",
        "outputs"
      ],
      "metadata": {
        "id": "O7jOAZHt0LGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly, this is much better!\n",
        "One needs to be careful with these things. It is often best to consult the official documentation, but be careful as this can also be stale and not up to date. For example, Gemma documentation still says that Gemma models are not using system isntruction as a separate chat block, but their own examples do have it:\n",
        "https://huggingface.co/google/gemma-3-1b-it\n",
        "\n"
      ],
      "metadata": {
        "id": "rW0O9jtQPnw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still, it is not ideal and now we can try to achieve better formatting with SFT. We just need to preprocess our data to be in the above mentioned format. Transformers templates can help:\n",
        "https://huggingface.co/docs/trl/main/en/dataset_formats#converting-a-conversational-dataset-into-a-standard-dataset"
      ],
      "metadata": {
        "id": "pHGO0Tk1QkGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have to apply the chat template for `Gemma-3` onto the conversations, and save it to `text`. We remove the `<bos>` token using removeprefix(`'<bos>'`) since we're finetuning. The Processor will add this token before training and the model expects only one."
      ],
      "metadata": {
        "id": "E3cIYhy8Qo-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")"
      ],
      "metadata": {
        "id": "JfBLFF_sQiUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = train_dataset[0]\n",
        "example"
      ],
      "metadata": {
        "id": "sCS6FhpZSNzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_example = tokenizer.apply_chat_template(\n",
        "    [{\"role\" : \"user\", \"content\" : example['question']},\n",
        "     {\"role\" : \"assistant\", \"content\" : example['answer']}],\n",
        "    add_generation_prompt=False,\n",
        "    tokenize=False,\n",
        "    #return_dict=True,\n",
        "    return_tensors=\"pt\",\n",
        ")#.to(model.device).to(torch.bfloat16) #this makes sense if we are returning tokens\n",
        "formatted_example"
      ],
      "metadata": {
        "id": "IDWYMigmSA_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using templates is convenient because we can easily choose if we want strings or tokens returned etc.\n",
        "But, what is happening behind the scenes can be done, of course, without templates as well:\n",
        "\n",
        "```\n",
        "def format_gsm8k_prompt(example):\n",
        "    \"\"\"Format GSM8K examples into a chat format for instruction tuning\"\"\"\n",
        "    question = example[\"question\"]\n",
        "    answer = example[\"answer\"]\n",
        "    \n",
        "    # Create a structured prompt\n",
        "    prompt = f\"\"\"<bos><start_of_turn>user\n",
        "    Solve this math problem step by step:\n",
        "\n",
        "    {question}<end_of_turn>\n",
        "    <start_of_turn>model\n",
        "    {answer}<end_of_turn><eos>\"\"\"\n",
        "    \n",
        "    return {\"text\": prompt}\n",
        "```\n"
      ],
      "metadata": {
        "id": "gN9Wk8xBXXaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analyzing the Dataset"
      ],
      "metadata": {
        "id": "zs_94rtXJzeE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65075730",
        "collapsed": true
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_answer_length_distribution(dataset_split, split_name):\n",
        "    \"\"\"\n",
        "    Calculates and plots the distribution of answer lengths for a dataset split.\n",
        "\n",
        "    Args:\n",
        "        dataset_split: A Hugging Face dataset split (e.g., train_dataset or test_dataset).\n",
        "        split_name (str): The name of the dataset split (e.g., \"Train\" or \"Test\").\n",
        "    \"\"\"\n",
        "    answer_lengths = []\n",
        "    for example in dataset_split:\n",
        "        answer = example['answer']\n",
        "        answer_lengths.append(len(answer))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(answer_lengths, bins=50, edgecolor='black')\n",
        "    plt.title(f\"Distribution of Answer Lengths ({split_name} Dataset)\")\n",
        "    plt.xlabel(\"Answer Length\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "\n",
        "plot_answer_length_distribution(train_dataset, \"Train\")\n",
        "plot_answer_length_distribution(test_dataset, \"Test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like we need to increase the number of tokens in the response if we are to expect from our model to be able to solve this tasks.\n",
        "For good results, we would need longer outputs.\n",
        "\n",
        "## Homework No.1:\n",
        "Calculate basic descriptive statistics for lengths of responses in our dataset (if you do now know which are common descriptive statistics, Google it!)\n",
        "\n",
        "## Homework No.2:\n",
        "Try using different types of prompts that we discussed (few shot prompt, Chain-of-Tought) with the non-trained model and observe the results."
      ],
      "metadata": {
        "id": "Jt2Kc0U8Q9Zj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Speeding up generation - Meet vLLM\n",
        "As we saw in the dataset analysis, we need longer outputs so that our model can realistically solve these math problems. But longer sequences mean longer time for generations.\n",
        "But luckily, we can significantly speed up the generation by using the vLLM library for fast inference. vLLM has a bunch of smart optimizations enabling much faster inference.\n",
        "For more info, visit https://github.com/vllm-project/vllm\n",
        "\n",
        "Note the `gpu_memory_utilization` parameter, this defines how much of a VRAM memory will be used for vLLM optimizations (paged attention etc.). Reduce this if you run into CUDA out of memory issues (e.g. if you also have a model loaded from Transformers).\n"
      ],
      "metadata": {
        "id": "m2PqcSOI0elQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "outputs = utils.generate_output(model, tokenizer, prompt, num_return_sequences=1, max_new_tokens=256)\n",
        "outputs\n"
      ],
      "metadata": {
        "id": "Awcgs38L0WHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Load the model with vLLM\n",
        "vanilla = \"unsloth/gemma-3-1b-it\"\n",
        "#trained_model = \"/content/drive/My Drive/...\"\n",
        "\n",
        "llm = LLM(model=vanilla, trust_remote_code=True, gpu_memory_utilization=0.75)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4qAQawart2rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm # checking if the model was initializes successfully in vllm engine"
      ],
      "metadata": {
        "id": "nUaBubkzI9oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define the sampling parameters. Remember, we had these before:\n",
        "\"\"\"default_kwargs = {\n",
        "        \"max_new_tokens\": 256,\n",
        "        \"do_sample\": True,\n",
        "        \"top_k\": 64,\n",
        "        \"top_p\": 0.95,\n",
        "        \"temperature\": 0.7,\n",
        "        \"num_return_sequences\": 1,\n",
        "        \"pad_token_id\": tokenizer.eos_token_id,\n",
        "        \"eos_token_id\": tokenizer.eos_token_id, # good practice to set this\n",
        "    }\n",
        "so let's use the same, so that our results are comparable.\n",
        "\"\"\"\n",
        "sampling_params = SamplingParams(temperature=1, top_p=0.95, top_k = 64, max_tokens=386)\n"
      ],
      "metadata": {
        "id": "W20By9e2x4KG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prepare your prompts\n",
        "prompts = [\n",
        "    preamble + instruction + test_dataset[0]['question'] + suffix,\n",
        "    preamble + instruction + test_dataset[1]['question'] + suffix,\n",
        "]\n"
      ],
      "metadata": {
        "id": "pM6Nn406QUyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    instruction + train_dataset[0]['question'],\n",
        "    instruction + train_dataset[1]['question'],\n",
        "]"
      ],
      "metadata": {
        "id": "sNwRE2C5ooOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts"
      ],
      "metadata": {
        "id": "Yk9-_zjKXmvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "vLLM reserves space on your GPU, no matter what the model size is (actual reserved space is controllable with the parameter where you can set a percentage for it). Let's check the status of out GPU utilization now:"
      ],
      "metadata": {
        "id": "-rX4iJ2A07q7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "sIG9KQA-uuSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Generate text\n",
        "outputs = llm.generate(prompts, sampling_params)\n"
      ],
      "metadata": {
        "id": "F_Dz8gdBuKga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oh, that's much faster!\n"
      ],
      "metadata": {
        "id": "Fb0MN0w81ONh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the outputs\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt}, Generated: {generated_text}\")"
      ],
      "metadata": {
        "id": "RiJVjHRIzvRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised Fine Tuning - SFT"
      ],
      "metadata": {
        "id": "0BGHj-YLZAhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the official Unsloth example of SFT for Gemma 3, check out this notebook: https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Gemma3_(4B).ipynb"
      ],
      "metadata": {
        "id": "MR1VFhlMkbZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now let's prepare our entire dataset for SFT. (let's stick with the templates):\n",
        "https://huggingface.co/docs/trl/main/en/dataset_formats#which-dataset-type-to-use"
      ],
      "metadata": {
        "id": "OxmAOLY9WS9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset_dataset = train_dataset.select(range(100))"
      ],
      "metadata": {
        "id": "Y5PBzCotdjQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reasoning_start = \"<start_working_out>\"\n",
        "reasoning_end   = \"<end_working_out>\"\n",
        "solution_start = \"<SOLUTION>\"\n",
        "solution_end = \"</SOLUTION>\"\n",
        "\n",
        "system_prompt = \\\n",
        "f\"\"\"You are given a problem.\n",
        "Think about the problem and provide your working out.\n",
        "Place it between {reasoning_start} and {reasoning_end}.\n",
        "Then, provide your solution between {solution_start}{solution_end}\\n\\n\"\"\"\n",
        "system_prompt"
      ],
      "metadata": {
        "id": "JTaWOfMYhI-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "CbsXLNnPH0_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'"
      ],
      "metadata": {
        "id": "H_3KINaVIQzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "def extract_final_answer(text: str, pattern: str) -> Optional[float]:\n",
        "    try:\n",
        "        if pattern.startswith('<') and pattern.endswith('>'):\n",
        "            # Extract tag name (e.g., 'answer' from '<answer>')\n",
        "            tag = pattern[1:-1]\n",
        "            # Look for <tag>number</tag>\n",
        "            match = re.search(f'<{tag}>\\s*([+-]?\\d*\\.?\\d+)\\s*</{tag}>', text)\n",
        "            if match:\n",
        "                return float(match.group(1))\n",
        "        else:\n",
        "            # Any pattern - look for the pattern followed by a number (other characters are allowed between a patern and a number)\n",
        "            escaped_pattern = re.escape(pattern)\n",
        "            match = re.search(f'{escaped_pattern}.*?([+-]?\\d*\\.?\\d+)', text) #*?([+-]?\\d*\\.?\\d+) would be for strict match\n",
        "            if match:\n",
        "                return float(match.group(1))\n",
        "\n",
        "    except (ValueError, AttributeError):\n",
        "        pass\n",
        "\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "wUejD0rdlsnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_final_answer(answer, '####')"
      ],
      "metadata": {
        "id": "MsUk1KsEITae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = '####'\n",
        "escaped_pattern = re.escape(pattern)\n",
        "re.search(f'{escaped_pattern}.*?([+-]?\\d*\\.?\\d+)', answer).group(0)"
      ],
      "metadata": {
        "id": "Hq8Xv-NoIb-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "escaped_pattern"
      ],
      "metadata": {
        "id": "f8CZedRKIlqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile -a {directory}/utils.py\n",
        "\n",
        "def format_gsm8k_sft(examples):\n",
        "    # examples is a dictionary where each key holds a list of items.\n",
        "    # We zip the lists for 'question' and 'answer' together to process them in pairs.\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            [\n",
        "                {\"role\": \"user\", \"content\": system_prompt + question},\n",
        "                {\"role\": \"assistant\", \"content\": reasoning_start + answer.split(re.search('\\#\\#\\#\\#.*?([+-]?\\d*\\.?\\d+)', answer).group(0))[0] + reasoning_end + solution_start + re.search('\\#\\#\\#\\#.*?([+-]?\\d*\\.?\\d+)', answer).group(1) + solution_end}\n",
        "            ],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        ).removeprefix('<bos>')\n",
        "        for question, answer in zip(examples['question'], examples['answer'])\n",
        "    ]\n",
        "    return {\"text\": texts}\n"
      ],
      "metadata": {
        "id": "AOaBnUK6WiDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sft_dataset = subset_dataset.map(format_gsm8k_sft, batched = True)\n",
        "sft_dataset_train = train_dataset.map(format_gsm8k_sft, batched = True, remove_columns=train_dataset.column_names)\n",
        "#sft_dataset_test = test_dataset.map(format_gsm8k_sft, batched = True, remove_columns=test_dataset.column_names)"
      ],
      "metadata": {
        "id": "VPDeIvZcfblT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft_dataset_train[2]"
      ],
      "metadata": {
        "id": "SIaCLFfse_lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft_dataset_train[3][\"text\"]"
      ],
      "metadata": {
        "id": "dTmMcGcPcY9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import wandb\n"
      ],
      "metadata": {
        "id": "1PR6PJnGY_5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "nf1hjafCeDYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = sft_dataset_train,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 600,\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"wandb\", # Use this for WandB etc\n",
        "        dataset_num_proc=2,\n",
        "        output_dir=f\"{directory}/outputs/sft_model\",\n",
        "        save_steps=200,\n",
        "        save_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "        per_device_eval_batch_size=1,\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "00mSjpUbZQln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs. This helps increase accuracy of finetunes!"
      ],
      "metadata": {
        "id": "TEPawDLtjDKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part = \"<start_of_turn>user\\n\",\n",
        "    response_part = \"<start_of_turn>model\\n\",\n",
        ")"
      ],
      "metadata": {
        "id": "RZqkSJTLh3J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify masking the instruction part is done! Let's print the 100th row again.  Notice how the sample only has a single `<bos>` as expected!"
      ],
      "metadata": {
        "id": "ZQ1jyrdtjd9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
      ],
      "metadata": {
        "id": "gXZF0v2ih3Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's print the masked out example - you should see only the answer is present\n"
      ],
      "metadata": {
        "id": "0fTF1gZjjp3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
      ],
      "metadata": {
        "id": "GqJsdKDqh3Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, finally, let's train our model!"
      ],
      "metadata": {
        "id": "XIa0De2zj3XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from google.colab import userdata\n",
        "\n",
        "# Access the stored secret\n",
        "wandb_api_key = userdata.get('wandb')\n",
        "wandb.login(key=wandb_api_key)\n",
        "\n",
        "# Initialize your wandb run and set the experiment name\n",
        "run = wandb.init(\n",
        "    project=\"gemma3-gsm8k-sft\",  # Replace with your project name\n",
        "    #name=\"experiment name\"     # Replace with your desired experiment name\n",
        ")\n"
      ],
      "metadata": {
        "id": "cro2IkIopdCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "f308NRZyh3BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats"
      ],
      "metadata": {
        "id": "N6XGGzRCh25-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(f\"{directory}/outputs/sft_model\")  # Saving to Drive\n",
        "tokenizer.save_pretrained(f\" {directory}/outputs/sft_model\")\n",
        "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "#Loading a model\n",
        "if False:\n",
        "    from unsloth import FastModel\n",
        "    model, tokenizer = FastModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing inference (as we had in the beginning)\n",
        "messages = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\"type\" : \"text\", \"text\" : \"What is the square root of 256?\",}]\n",
        "}]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n"
      ],
      "metadata": {
        "id": "UXrxVbZaPjq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 64, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ],
      "metadata": {
        "id": "S9qyjSVVPc-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "Unsloth also supports saving to `float16` directly for deployment! We save it in the folder `gemma-3-finetune`. Set `if False` to `if True` to let it run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained_merged(f\"{directory}/outputs/sft_model_deploy\", tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6O48DbNIAr0"
      },
      "source": [
        "If you want to upload / push to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV-CiKPrIFG0"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_merged(\n",
        "        \"HF_ACCOUNT/gemma-3-finetune\", tokenizer,\n",
        "        token = \"hf_...\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference and Evaluation"
      ],
      "metadata": {
        "id": "H3njjRdll6M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt"
      ],
      "metadata": {
        "id": "FndBjghZF9SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reasoning_start = \"<start_working_out>\"\n",
        "reasoning_end   = \"<end_working_out>\"\n",
        "solution_start = \"<SOLUTION>\"\n",
        "solution_end = \"</SOLUTION>\"\n",
        "\n",
        "system_prompt = \\\n",
        "f\"\"\"You are given a problem.\n",
        "Think about the problem and provide your working out.\n",
        "Place it between {reasoning_start} and {reasoning_end}.\n",
        "Then, provide your solution between {solution_start}{solution_end}\\n\\n\"\"\"\n",
        "system_prompt"
      ],
      "metadata": {
        "id": "AR-hBLcHGIIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preamble = \"<start_of_turn>user\\n\"\n",
        "suffix = \"<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "prompt = preamble + system_prompt + train_dataset[0]['question'] + suffix\n",
        "#prompt = train_dataset[0]['question']\n",
        "prompt"
      ],
      "metadata": {
        "id": "tU8Ufz_emcRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "id": "bBwz8Y4VFCsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": train_dataset[0]['question']},\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    tokenize = False,\n",
        ")\n",
        "print(text)"
      ],
      "metadata": {
        "id": "XKcpgCi7JBqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = generate_output(model, tokenizer, prompt, num_return_sequences=5, max_new_tokens=256)\n",
        "#outputs = utils.generate_output(model, tokenizer, prompt, num_return_sequences=5, max_new_tokens=256)\n",
        "\n",
        "outputs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "osLt9_TZHh2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the trained model\n"
      ],
      "metadata": {
        "id": "aLLcf6Hnob-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[3]"
      ],
      "metadata": {
        "id": "v0BKD3f7GuY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = \"Final Answer:\"\n",
        "match = re.search(f'{pattern}.*?([+-]?\\d*\\.?\\d+)', outputs[3])\n",
        "match.group(1)"
      ],
      "metadata": {
        "id": "w7I4qI0sIKSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_final_answer(outputs[0],\"<SOLUTION>\")"
      ],
      "metadata": {
        "id": "yv0YXHrNGzz_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's write evaluation function. To make it more versatile, we will pass separate patterns for dataset and for model. (so that we can use it later to evaluate Reasoning model that will have different pattern)"
      ],
      "metadata": {
        "id": "ePJyjpPcJyvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for the GRPO trained model\n",
        "reasoning_start = \"<start_working_out>\"\n",
        "reasoning_end   = \"<end_working_out>\"\n",
        "solution_start = \"<SOLUTION>\"\n",
        "solution_end = \"</SOLUTION>\"\n",
        "\n",
        "system_prompt = \\\n",
        "f\"\"\"You are given a problem.\n",
        "Think about the problem and provide your working out.\n",
        "Place it between {reasoning_start} and {reasoning_end}.\n",
        "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
        "system_prompt"
      ],
      "metadata": {
        "id": "Dt7UxEEzNUP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_gsm8k_eval_prompt(example, pattern):\n",
        "    \"\"\"Format GSM8K examples into a chat format for evaluation\"\"\"\n",
        "    question = example[\"question\"]\n",
        "    answer = example[\"answer\"]\n",
        "\n",
        "    ## Create a structured prompt:\n",
        "    # Using the 'system' prompt\n",
        "    #prompt = f\"\"\"<bos><start_of_turn>system\\n{system_prompt}<start_of_turn>user\\n{question}\\n<end_of_turn><start_of_turn>model\"\"\"\n",
        "    #Without the 'system' prompt, i.e. prepending it to 'user' message (as in Gemma documentation)\n",
        "    prompt = f\"\"\"<bos><start_of_turn>user\\n{system_prompt}\\n{question}\\n<end_of_turn><start_of_turn>model\"\"\"\n",
        "    # Without structuring - just system prompt + plain text\n",
        "    #prompt = f\"\"\"{system_prompt} {question}\\n\"\"\"\n",
        "\n",
        "\n",
        "    ground_truth = extract_final_answer(answer, pattern)\n",
        "\n",
        "    return {\"prompt\": prompt, 'ground_truth': ground_truth}"
      ],
      "metadata": {
        "id": "hPcCt6tm6heD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(format_gsm8k_eval_prompt(train_dataset[0], '####'))"
      ],
      "metadata": {
        "id": "KpiV05Fq7Ms5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_gm8k_test_dataset = test_dataset.map(format_gsm8k_eval_prompt, fn_kwargs={'pattern':'####'})"
      ],
      "metadata": {
        "id": "3im2stR976ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_gm8k_test_dataset[0]"
      ],
      "metadata": {
        "id": "W3p4jXBT8M7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our evaluation function will work on batches of data."
      ],
      "metadata": {
        "id": "576-2C5IOSeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import re\n",
        "from typing import Dict, Any, Optional\n",
        "\n",
        "def evaluate_gsm8k_batch(\n",
        "    model: AutoModelForCausalLM,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    test_dataset,\n",
        "    model_pattern: str,\n",
        "    max_new_tokens: int,\n",
        "    temperature: float,\n",
        "    batch_size: int,\n",
        "    max_samples: Optional[int] = None,\n",
        "    top_p: float = 0.95,\n",
        "    top_k: int = 64,\n",
        "    verbose: bool = False,\n",
        ") -> Dict[str, Any]:\n",
        "\n",
        "    if max_samples:\n",
        "        test_dataset = test_dataset.select(range(min(max_samples, len(test_dataset))))\n",
        "\n",
        "    test_dataset = test_dataset.map(format_gsm8k_eval_prompt, fn_kwargs={'pattern':'####'})\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device).eval()\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = 'left'\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    generation_kwargs = {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "        \"top_k\": top_k,\n",
        "        \"do_sample\": temperature > 0,\n",
        "        \"pad_token_id\": tokenizer.pad_token_id,\n",
        "    }\n",
        "\n",
        "    print(f\"Evaluating {len(test_dataset)} samples on {device}...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(test_dataset), batch_size), desc=\"Batch Inference\"):\n",
        "            batch_dataset = test_dataset[i:i + batch_size]\n",
        "\n",
        "            batch_prompts = batch_dataset['prompt']\n",
        "            batch_gts = batch_dataset['ground_truth']\n",
        "            batch_questions = batch_dataset.get('question', ['N/A'] * len(batch_dataset)) # Safely get questions\n",
        "\n",
        "            inputs = tokenizer(batch_prompts, padding=True, return_tensors=\"pt\").to(device)\n",
        "            outputs = model.generate(**inputs, **generation_kwargs)\n",
        "            solution_texts = tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "            for j, solution_str in enumerate(solution_texts):\n",
        "                pred_answer = extract_final_answer(solution_str, model_pattern)\n",
        "                is_correct = (float(pred_answer) == float(batch_gts[j])) if pred_answer is not None else False\n",
        "\n",
        "                if verbose:\n",
        "                    print(f\"\\n--- Example ---\\n\"\n",
        "                          f\"Prompt: {batch_prompts[j]}\\n\"\n",
        "                          f\"Solution: {solution_str}\\n\"\n",
        "                          f\"Ground Truth: {batch_gts[j]}\\n\"\n",
        "                          f\"Predicted Answer: {pred_answer}\\n\"\n",
        "                          f\"Correct: {is_correct}\")\n",
        "\n",
        "                all_results.append({\n",
        "                    'question': batch_questions[j],\n",
        "                    'predicted_answer': pred_answer,\n",
        "                    'ground_truth_answer': batch_gts[j],\n",
        "                    'solution_str': solution_str,\n",
        "                    'is_correct': is_correct\n",
        "                })\n",
        "\n",
        "    correct_count = sum(r['is_correct'] for r in all_results)\n",
        "    total_count = len(all_results)\n",
        "    accuracy = correct_count / total_count if total_count > 0 else 0\n",
        "\n",
        "    print(f\"\\nFinal accuracy: {accuracy:.2%}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'correct': correct_count,\n",
        "        'total': total_count,\n",
        "        'detailed_results': all_results\n",
        "    }"
      ],
      "metadata": {
        "id": "6pBkq_yvqivo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nStarting evaluation...\")\n",
        "results = evaluate_gsm8k_batch(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    test_dataset=test_dataset,\n",
        "    #test_pattern='####',  # GSM8K uses #### pattern\n",
        "    model_pattern='<SOLUTION>',  # Whatever our mode uses, e.g. <answer> tags\n",
        "    max_new_tokens=384,\n",
        "    temperature=1,\n",
        "    batch_size=4,\n",
        "    max_samples=12,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"Accuracy: {results['accuracy']:.2%}\")\n",
        "print(f\"Correct: {results['correct']}\")\n",
        "print(f\"Total: {results['total']}\")\n",
        "\n",
        "# Optionally, print some detailed results\n",
        "# print(\"\\nFirst 5 detailed results:\")\n",
        "# for i, res in enumerate(results['detailed_results'][:5]):\n",
        "#     print(f\"--- Example {i+1} ---\")\n",
        "#     print(f\"Question: {res['question']}\")\n",
        "#     print(f\"True Answer: {res['ground_truth_answer']}\")\n",
        "#     print(f\"Model Raw Output: {res['solution_str']}\")\n",
        "#     print(f\"Model Extracted Answer: {res['predicted_answer']}\")\n",
        "#     print(f\"Correct: {res['is_correct']}\")\n"
      ],
      "metadata": {
        "id": "EBFFd3PYqqU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is so called pass@1 evaluation. Usually, researchers report pass@k accuracy. One would query model for several times and then measure success rate."
      ],
      "metadata": {
        "id": "Vy8Y5rRif3dO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vLLM Evaluation"
      ],
      "metadata": {
        "id": "Y99BBmRbJgPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is slow (batching can help), but let's use our friend vLLM to make a blazingly fast evaluation.\n",
        "\n",
        "Note: for switching models in vLLM on Google Colab, a reset of Runtime is often the best bet\n",
        "While it might seem intuitive to simply release one model from memory and load another, vLLM's current architecture does not offer a straightforward or reliable mechanism for \"hot-swapping\" models within the same session.\n",
        "\n",
        "At the heart of this limitation is vLLM's highly optimized memory management. To achieve its impressive inference speeds, vLLM pre-allocates a significant portion of the available GPU memory for the loaded model and its associated key-value (KV) cache. The library does not, as of now, provide a simple function to completely clear a loaded model and its memory footprint to make way for a new one."
      ],
      "metadata": {
        "id": "cUd32DadPzFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "def evaluate_gsm8k_vllm(\n",
        "    model,\n",
        "    test_dataset,\n",
        "    test_pattern: str,\n",
        "    model_pattern: str,\n",
        "    max_new_tokens,\n",
        "    temperature,\n",
        "    max_samples=None) -> dict:\n",
        "    if max_samples:\n",
        "        test_dataset = test_dataset.select(range(min(max_samples, len(test_dataset))))\n",
        "\n",
        "    print(f\"Evaluating {len(test_dataset)} samples...\")\n",
        "\n",
        "    print(f\"Preprocessing {len(test_dataset)} samples...\")\n",
        "    test_dataset = test_dataset.map(format_gsm8k_eval_prompt, fn_kwargs={'pattern':test_pattern})\n",
        "\n",
        "    llm = model\n",
        "    # Use Gemma's validation sampling parameters, these could also be arguments\n",
        "    # for the entire function\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=temperature,\n",
        "        top_p=0.95,\n",
        "        top_k=64,\n",
        "        max_tokens=max_new_tokens,\n",
        "    )\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_results = []\n",
        "\n",
        "    # Prepare all prompts for bulk inference\n",
        "    prompts = [item['prompt'] for item in test_dataset]\n",
        "    print(len(prompts))\n",
        "\n",
        "    # Perform bulk inference once using vLLM\n",
        "    print(\"\\nRunning inference...\")\n",
        "    outputs = llm.generate(prompts, sampling_params)\n",
        "    print(\"Inference complete.\")\n",
        "\n",
        "    try:\n",
        "        # Iterate through the generated outputs and corresponding dataset items\n",
        "        for output, item in tqdm(zip(outputs, test_dataset), total=len(test_dataset), desc=\"Processing results\"):\n",
        "            solution_str = output.outputs[0].text\n",
        "\n",
        "            # Debugging prints (TODO: consider making these optional with a 'verbose'' flag as in the previous function)\n",
        "            print('\\n--- Example ---')\n",
        "            print('Prompt: ' + item['prompt'])\n",
        "            print('Solution: ' + solution_str)\n",
        "            #print('Solution (last 200 chars): ' + solution_str[-200:])\n",
        "            print('GT Full Answer (last 100 chars): ' + item['answer'][-100:])\n",
        "            print(f\"Final ground truth answer: {item['ground_truth']}\")\n",
        "\n",
        "            try:\n",
        "                # Attempt to extract the predicted final answer\n",
        "                pred_final_answer = extract_final_answer(solution_str, model_pattern)\n",
        "                print(f\"Predicted answer: {pred_final_answer}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to extract final answer from generated answer: {e}\")\n",
        "                pred_final_answer = None # Set to None if extraction fails\n",
        "\n",
        "            # Check if the prediction is correct\n",
        "            if pred_final_answer is not None: # Only compare if an answer was extracted\n",
        "                try:\n",
        "                    if float(pred_final_answer) == float(item['ground_truth']):\n",
        "                        correct += 1\n",
        "                    else:\n",
        "                        print(f\"Incorrect prediction for question: {item.get('original_question', 'N/A')}\")\n",
        "                except ValueError: # Handle cases where conversion to float fails\n",
        "                    print(f\"Type conversion error for comparison: pred='{pred_final_answer}', gt='{item['ground_truth']}'\")\n",
        "            else:\n",
        "                print(f\"No valid predicted answer extracted for question: {item.get('original_question', 'N/A')}\")\n",
        "\n",
        "\n",
        "            all_results.append({\n",
        "                'question': item['question'],\n",
        "                'predicted_answer': pred_final_answer,\n",
        "                'ground_truth_answer': item['ground_truth'],\n",
        "                'solution_str': solution_str\n",
        "            })\n",
        "            total += 1\n",
        "\n",
        "        accuracy = correct / total if total > 0 else 0\n",
        "        print(f\"\\nFinal accuracy: {accuracy:.2%}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during evaluation loop: {e}\")\n",
        "        if total > 0:\n",
        "            print(f\"Partial results available for {total} examples\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up CUDA memory\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        else:\n",
        "            print(\"CUDA not available, skipping empty_cache()\")\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'correct': correct,\n",
        "        'total': total,\n",
        "        'detailed_results': all_results\n",
        "    }"
      ],
      "metadata": {
        "id": "Q98kbTktFVyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = evaluate_gsm8k_vllm(\n",
        "    model=llm,\n",
        "    test_dataset= test_dataset,\n",
        "    test_pattern='####',  # GSM8K uses #### pattern\n",
        "    model_pattern='<SOLUTION>',  # Whatever our mode uses, e.g. <answer> tags\n",
        "    max_new_tokens=768,\n",
        "    temperature=0.1,\n",
        "    max_samples=100  # Evaluate on first x samples for testing\n",
        ")"
      ],
      "metadata": {
        "id": "wCEpT3mCBplV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "id": "Z3FJ256lFUS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utils.generate_output(model, tokenizer, prompt, num_return_sequences=1)"
      ],
      "metadata": {
        "id": "PkJRagdfB5fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reinforcement Learning\n",
        "Let's take our model's reasoning capabilities to the next level! We will do some RL (Reinforcement Learning), or more specifically - GRPO (Group Relative Policy Optimization).\n",
        "Following block borrows code from https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/HuggingFace%20Course-Gemma3_(1B)-GRPO.ipynb"
      ],
      "metadata": {
        "id": "TtU8G8CgkyDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to format our dataset?\n",
        "For different tasks we will need to format our dataset differently.\n",
        "Luckily, there is this nice TRL documentation page:\n",
        "https://huggingface.co/docs/trl/main/en/dataset_formats#which-dataset-type-to-use"
      ],
      "metadata": {
        "id": "hBcF394OXeQv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5I3BCkViwuC"
      },
      "outputs": [],
      "source": [
        "def extract_hash_answer(text):\n",
        "    if \"####\" not in text: return None\n",
        "    return text.split(\"####\")[1].strip()\n",
        "extract_hash_answer(test_dataset[0][\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiFNBLyytPCD"
      },
      "source": [
        "We now create a system prompt which can be customized. We add 4 extra symbols for working out or thinking / reasoning sections and a final answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHjiV3kGi8Y9"
      },
      "outputs": [],
      "source": [
        "reasoning_start = \"<start_working_out>\"\n",
        "reasoning_end   = \"<end_working_out>\"\n",
        "solution_start = \"<SOLUTION>\"\n",
        "solution_end = \"</SOLUTION>\"\n",
        "\n",
        "system_prompt = \\\n",
        "f\"\"\"You are given a problem.\n",
        "Think about the problem and provide your working out.\n",
        "Place it between {reasoning_start} and {reasoning_end}.\n",
        "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
        "system_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFRYlk9ntYTm"
      },
      "source": [
        "Let's map the dataset! and see the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tkTF5Hmlhl-"
      },
      "outputs": [],
      "source": [
        "rl_dataset = test_dataset.map(lambda x: {\n",
        "    \"prompt\" : [\n",
        "        {\"role\": \"user\", \"content\": preamble + system_prompt + x[\"question\"] + suffix}, # Note the Gemma's treatment of system prompt\n",
        "       #{\"role\": \"user\",   \"content\": x[\"question\"] + suffix},\n",
        "    ],\n",
        "    \"answer\": extract_hash_answer(x[\"answer\"]),\n",
        "})\n",
        "rl_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6MsfbGUtja0"
      },
      "source": [
        "We create a regex format to match the reasoning sections and answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5X6oDNDn6Zj"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "match_format = re.compile(\n",
        "    rf\"^[\\s]{{0,}}\"\\\n",
        "    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\\\n",
        "    rf\"{solution_start}(.+?){solution_end}\"\\\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME3-UVc6tnYP"
      },
      "source": [
        "We verify it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVvrKUBEtoQD"
      },
      "outputs": [],
      "source": [
        "match_format.search(\n",
        "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
        "    \"<SOLUTION>2</SOLUTION>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qglh2OxpuQzK"
      },
      "source": [
        "We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8MPYPvvo1ri"
      },
      "outputs": [],
      "source": [
        "def match_format_exactly(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Match if format is seen exactly!\n",
        "        if match_format.search(response) is not None: score += 3.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqnEZ4msuZyZ"
      },
      "source": [
        "If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LlYVZjdpij9"
      },
      "outputs": [],
      "source": [
        "def match_format_approximately(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Count how many keywords are seen - we penalize if too many!\n",
        "        # If we see 1, then plus some points!\n",
        "        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n",
        "        score += 0.5 if response.count(reasoning_end)   == 1 else -0.5\n",
        "        score += 0.5 if response.count(solution_start)  == 1 else -0.5\n",
        "        score += 0.5 if response.count(solution_end)    == 1 else -0.5\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBwDVDxtuhWm"
      },
      "source": [
        "Finally, we want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnKYp_IYqFr2"
      },
      "outputs": [],
      "source": [
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_format.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        score = 0\n",
        "        if guess is None:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        # Correct answer gets 3 points!\n",
        "        if guess == true_answer:\n",
        "            score += 3.0\n",
        "        # Match if spaces are seen\n",
        "        elif guess.strip() == true_answer.strip():\n",
        "            score += 1.5\n",
        "        else:\n",
        "            # We also reward it if the answer is close via ratios!\n",
        "            # Ie if the answer is within some range, reward it!\n",
        "            try:\n",
        "                ratio = float(guess) / float(true_answer)\n",
        "                if   ratio >= 0.9 and ratio <= 1.1: score += 0.5\n",
        "                elif ratio >= 0.8 and ratio <= 1.2: score += 0.25\n",
        "                else: score -= 1.0 # Penalize wrong answers\n",
        "            except:\n",
        "                score -= 0.5 # Penalize\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvOYCf1Ly83w"
      },
      "source": [
        "Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtFAX3_xy77b"
      },
      "outputs": [],
      "source": [
        "match_numbers = re.compile(\n",
        "    rf\"{solution_start}.*?([\\d\\.]{{1,}})\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "match_numbers.findall(\"<SOLUTION>  0.34  </SOLUTION>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: Fix bug in extraction of numbers to catch negative numbers in responses"
      ],
      "metadata": {
        "id": "j9Gwb2rdaJpk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqWply0z0DrP"
      },
      "outputs": [],
      "source": [
        "def check_numbers(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_numbers.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    # Seems like the following line misses to extract negative numbers\n",
        "    print('*'*20, f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        if guess is None:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "        # Convert to numbers\n",
        "        try:\n",
        "            true_answer = float(true_answer.strip())\n",
        "            guess       = float(guess.strip())\n",
        "            scores.append(1.5 if guess == true_answer else 0.0)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux6iqP7z5YOo"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptqkXK2D4d6p"
      },
      "outputs": [],
      "source": [
        "max_prompt_length = 256 # We should do the analysis of question lengths as we did for answers (and account for system instruction and our custom formatting!)\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"adamw_torch_fused\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
        "    num_generations = 4, # Decrease if out of memory\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_seq_length - max_prompt_length,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 100,\n",
        "    save_steps = 50,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"wandb\",\n",
        "    output_dir = f\"{directory}/outputs/grpo_model\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your wandb run and set the experiment name\n",
        "run = wandb.init(\n",
        "    project=\"gemma3-gsm8k-grpo\",  # Replace with your project name\n",
        "    #name=\"experiment name\"     # Replace with your desired experiment name\n",
        ")"
      ],
      "metadata": {
        "id": "UEpO6H1gU48C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "\n",
        "**[Marko's edit: Unless you use SFT warmup, then your model will start collecting formatting rewards early!]**\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzOuSVCL_GA9"
      },
      "outputs": [],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = rl_dataset,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcz_lpbVC92"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    #{\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": system_prompt + \"What is the sqrt of 101?\"},\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    tokenize = False,\n",
        ")\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 256, # Increase for longer outputs!\n",
        "    # Recommended Gemma-3 settings!\n",
        "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ],
      "metadata": {
        "id": "-3Dma8YMakRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(f\"{directory}/outputs/grpo_model\")  # Local saving\n",
        "tokenizer.save_pretrained(f\"{directory}/outputs/grpo_model\")\n",
        "# model.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"HF_ACCOUNT/gemma-3\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly for deployment! We save it in the folder `gemma-3-finetune`. Set `if False` to `if True` to let it run!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained_merged(f\"{directory}/outputs/grpo_model_deploy_100_steps\", tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRrqfyaRaXRL"
      },
      "source": [
        "If you want to upload / push to your Hugging Face account, set `if False` to `if True` and add your Hugging Face token and upload location!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-B07m_HC5i7"
      },
      "outputs": [],
      "source": [
        "if False: # Change to True to upload finetune\n",
        "    model.push_to_hub_merged(\n",
        "        \"HF_ACCOUNT/gemma-3-finetune\", tokenizer,\n",
        "        token = \"hf_...\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusion and Resources\n",
        "\n",
        "That's it, congratulations! If everything worked as it should, you now have your own Reasoning model!\n",
        "There may be some compatibiliy issues or hiccups in following this notebook, but this is normal in a fast-paced, always changing open-source world.\n",
        "\n",
        "For more notebooks with examples of finetuning various models with Unsloth, visit their website: https://unsloth.ai/blog\n",
        "\n",
        "Here are some interesting resources to learn more in-depth about reasoning language models:\n",
        "\n",
        "*   https://magazine.sebastianraschka.com/p/understanding-reasoning-llms\n",
        "*   https://epichka.com/blog/2025/grpo/\n",
        "\n"
      ],
      "metadata": {
        "id": "1gPwuHK8EcS9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wBJ66Ubegwvg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}