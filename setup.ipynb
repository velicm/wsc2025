{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Welcome to the 'Reasoning' with LLMs Workshop!\n",
        "\n",
        "Author: Marko Velic\n",
        "\n",
        "June 2025\n",
        "\n",
        "##Introduction\n",
        "In this workshop we will explore reasoning with LLMs. During the workshop we will gradually build our own library for learning, exploring and playing with Reasoning (or Thinking) LLMs. We will work through notebooks, but at the same time, we will export important pieces of code into the separate .py files and thus build a small reasoning library that is easy to use later and build upon.\n",
        "\n",
        "To avoid any surprises on premises, please run all cells from this notebook in advace. This will download a Gemma3 open-weights model and save it on your Google Drive. A model will take up approximately one GB of space. We will use this model as a starting point on which we will build our 'reasoning' model. It will then load that same model from Drive and try it out.\n",
        "\n",
        "For this notebook to work, you need to run in the Runtime that has NVIDIA GPU. Don't worry - it is just a few click away and free (thank you Google Colab :-)). You just need to click on the Connection Options in the upper-right corner of the screen (or just click on the `Runtime` item in the main Menu) and then click on the `Change runtime type`. Select `T4 GPU` or `L4 GPU` option."
      ],
      "metadata": {
        "id": "7qp56WUrLK_S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBBxY57dF8MG"
      },
      "outputs": [],
      "source": [
        "# --- Initial Project Setup ---\n",
        "# This cell creates the directory structure for our project.\n",
        "# We'll be populating these files as we go through the workshop.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Add the path to your project folder in Drive to Python's search path\n",
        "directory = '/content/drive/MyDrive/Colab_Notebooks/llm_workshop'\n",
        "sys.path.append(directory)\n",
        "file_path = f\"{directory}/utils_hello_drive.py\"\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Subdirs\n",
        "!mkdir {directory}/notebooks\n",
        "!mkdir {directory}/scripts\n",
        "!mkdir {directory}/src\n",
        "!mkdir {directory}/outputs\n",
        "!mkdir {directory}/outputs/sft_model\n",
        "!mkdir {directory}/outputs/grpo_model\n",
        "!mkdir {directory}/models\n"
      ],
      "metadata": {
        "id": "3E6tPxIBl8vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the __init__.py file to make 'src' a Python package\n",
        "!touch {directory}/src/__init__.py\n"
      ],
      "metadata": {
        "id": "rUpjYb9Mo11V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {directory}/src/utils.py"
      ],
      "metadata": {
        "id": "Uv5INZAHrfeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's try to put some .py file in out ```src``` folder and then try to use it in our notebook with ```import```"
      ],
      "metadata": {
        "id": "wGgZGUgGscO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile  {directory}/hello_from_drive.py\n",
        "\n",
        "def hello_world():\n",
        "  print(\"Hello from our python scrip in Drive.py!\")"
      ],
      "metadata": {
        "id": "pPkdgfCBpM7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hello_from_drive\n",
        "\n",
        "hello_from_drive.hello_world()"
      ],
      "metadata": {
        "id": "ZG27hODts-3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install uv"
      ],
      "metadata": {
        "collapsed": true,
        "id": "r-ylvP1xu-Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install unsloth vllm\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "joxcogGSupe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7V9xex0teU7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {directory}/utils.py\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "def load_model(model_name, **model_kwargs) -> FastLanguageModel:\n",
        "\n",
        "  default_kwargs = {\n",
        "    \"max_seq_length\": 1024,  # Can increase for longer reasoning traces\n",
        "    \"load_in_4bit\": True,  # False for LoRA 16bit\n",
        "    \"fast_inference\": True,  # Enable vLLM fast inference\n",
        "    \"max_lora_rank\": 32, # Larger rank = smarter, but slower\n",
        "    \"gpu_memory_utilization\": 0.6,  # Reduce if out of memory\n",
        "  }\n",
        "\n",
        "  default_kwargs.update(model_kwargs)\n",
        "  lora_rank = default_kwargs['max_lora_rank']\n",
        "\n",
        "  model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "      model_name= model_name,\n",
        "      **default_kwargs,\n",
        "  )\n",
        "\n",
        "  model = FastLanguageModel.get_peft_model(\n",
        "      model,\n",
        "      r=lora_rank,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "      target_modules=[\n",
        "          \"q_proj\",\n",
        "          \"k_proj\",\n",
        "          \"v_proj\",\n",
        "          \"o_proj\",\n",
        "          \"gate_proj\",\n",
        "          \"up_proj\",\n",
        "          \"down_proj\",\n",
        "      ],  # Remove QKVO if out of memory\n",
        "      lora_alpha=lora_rank,\n",
        "      use_gradient_checkpointing=\"unsloth\",  # Enable long context finetuning\n",
        "      random_state=3407,\n",
        "  )\n",
        "  return model, tokenizer\n"
      ],
      "metadata": {
        "id": "c4ookswmtX3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "model, tokenizer = utils.load_model(\"unsloth/gemma-3-1b-it\")\n"
      ],
      "metadata": {
        "id": "mefPVKOeNK_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(f\"{directory}/models/gemma-3-1b-it\")\n",
        "tokenizer.save_pretrained(f\"{directory}/models/gemma-3-1b-it\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's try to load a model from Drive and generate some text."
      ],
      "metadata": {
        "id": "79yQhUdgK-Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " model_from_drive, tokenizer_from_drive = utils.load_model (f\"{directory}/models/gemma-3-1b-it\")\n"
      ],
      "metadata": {
        "id": "kzaPLSAuKXQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time in a land far, far away, there lived a\"\n",
        "inputs = tokenizer_from_drive(prompt, return_tensors=\"pt\").to(model_from_drive.device)\n"
      ],
      "metadata": {
        "id": "lgnOwE9e4xWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model_from_drive.generate(\n",
        "    **inputs,\n",
        "    max_length=50,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.7,\n",
        "    output_scores=True,\n",
        "    return_dict_in_generate=True,\n",
        "    num_return_sequences=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "AeHt9JG95x70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the generated text\n",
        "generated_text = tokenizer_from_drive.decode(outputs[0][0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "ot5b-vhD7MuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zyPP_o3TKt6V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}